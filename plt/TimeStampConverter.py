import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sb
import math
import matplotlib.gridspec as gridspec

# Interval at witch we compare the progress of the agents

def process_data_line(num_agents, repeats, Interval , filename, processed_filename):
    """
        Merges all the TimeStamp files by listing for every agent how many updates it completed
        at every timepoint(Interval)
    :param num_agents:
    :param repeats:
    :param Interval: Interval at witch we compare the progress of the agents
    :param filename: Formatted file name, {0} for the number of Agents, {1} number of repeats, {2} agent nr.
    :param processed_filename: Formatted file name, {0} for the number of Agents, {1} number of repeats.
    :return:
    """

    #Each Iteration is processed on each own
    for i in range(0, repeats):

        list_of_lists = []
        # we process all the seperate Data form each run and each seperate agent into one file
        # for each run
        for k in range(1,num_agents+1):
            data = pd.read_csv(filename.format(num_agents,i+1,k))
            #get the Time data form the DataFrame as an numpy array
            list_of_lists.append(data.get_values()[:, 1])
        print("start processing for #agents:", num_agents, "and", i,"th iteration")
        #A pointer for each numpy array we collected above
        pointers = [0] * num_agents
        # get the min of the first entry across the agents
        min_ = (list_of_lists[0])[0]
        for l,p in enumerate(pointers):
            if (list_of_lists[l])[p] <= min_:
                min_ = (list_of_lists[l])[p]
        counter = 0.0
        time = min_

        #each agent did a variant amount of updates
        length_list = [list_of_lists[i].shape[0] for i in range(0,len(list_of_lists))]

        #get the max of the first entry across the agents
        min_ = (list_of_lists[0])[0]
        max_ = 0
        for l in range(0, len(list_of_lists)):
            if (list_of_lists[l])[-1] >= max_:
                max_ = (list_of_lists[l])[-1] # -1 taks the last element in the list

        #approzimate length of the processed DataFram
        size = math.ceil((max_ - min_) / Interval) #round offs may happen here -> postprocessing to shorten the DF
        size = math.ceil(size *1.1) #buffer, run into the probelm that it didn't finish for 10 agents
        print("size", size)
        processed = np.zeros((size,num_agents+1))

        total_sum = sum(length_list)

        #building the processed DF
        for itr in range(0,int(size)):
            #the intervall we look at now
            time = time + Interval
            processed[itr][0] = counter
            #going through all agents in this intervall and increasing the pointers if they did an update during it
            if np.sum(pointers) < ( total_sum- num_agents): # not efficent summing every iteration !!!!!!, But postprocessing is fast enough already
                for l,p in enumerate(pointers):
                    if p < length_list[l]- 1:
                        if (list_of_lists[l])[p] <= time:
                            pointers[l] = pointers[l]+1
                            processed[itr][l+1] = pointers[l]
                        else:
                            processed[itr][l+1] = pointers[l]
                    else:
                        processed[itr][l + 1] = pointers[l]
                counter = counter + Interval
            else:
                print("break")
                size = itr
                break
        #Writing the DF
        cols = ["Time"].append(["Agent"+str(nr)+"_Itr" for nr in range(0,num_agents)])
        df = pd.DataFrame(processed[0:size,:], columns= cols )
        df.to_csv(processed_filename.format(num_agents,i))


def process_data_minmax(Agents, repeats, processed_filename, minmax_filename):
    """
        Takes the data generated by the process_data_line function calculates for every timepoint the difference
        between the slowest and fastest agent
    :param Agents:
    :param repeats:
    :param processed_filename: Formatted file name, {0} for the number of Agents, {1} number of repeats.
    :param minmax_filename: Formatted file name, {0} for the number of Agents, {1} number of repeats.
    :return:
    """
    #Each Iteration is processed on each own
    for i in Agents:
        for j in range(0, repeats):
            list_of_lists = []
            data = pd.read_csv(processed_filename.format(i,j))
            values = data.get_values()

            #get the Time data form the DataFrame as an numpy array
            times = values[:,1]
            for k in range(2, i+2):
                list_of_lists.append(values[:, k])

            length = list_of_lists[0].shape[0]
            processed = np.zeros((length,2))

            for c in range(0,length):
                min_ = (list_of_lists[0])[c]
                max_ = min_
                for l in list_of_lists: #wastefull a the first already covered
                    if l[c] < min_:
                        min_ = l[c]
                    if l[c] > max_:
                        max_ = l[c]
                processed[c,1] = max_-min_

            processed[:,0] = times
            #Writing the DF
            cols = ["Times", "Diff"]
            df = pd.DataFrame(processed[0:length,:], columns= cols )
            df.to_csv(minmax_filename.format(i,j))


def process_data_median(Agents, repeats, processed_filename, minmax_filename):
    """
        Takes the data generated by the process_data_line function calculates for every timepoint the difference
        between the fastest and median agent. If the Number of agents is even then the median is the average of the
        two in the middle.
        Appends two columns to the MINMAX file generated by process_data_minmax
        """
    #Each Iteration is processed on each own
    for i in Agents:
        for j in range(0, repeats):
            list_of_lists = []
            data = pd.read_csv(processed_filename.format(i,j))
            values = data.get_values()
            #get the Time data form the DataFrame as an numpy array
            times = values[:,1]
            for k in range(2, i+2):
                list_of_lists.append(values[:, k])

            length = list_of_lists[0].shape[0]
            processed = np.zeros((length,3))

            middle = i // 2

            for c in range(0,length):
                list_at_step = []
                for l in list_of_lists: #wastefull a the first already covered
                    list_at_step.append(l[c])
                list_at_step.sort()
                # print(list_at_step)

                if i >1:

                    if i % 2 == 1:
                        median = list_at_step[middle] #rounded down is correct because list starts form 0

                    else:
                        median = (list_at_step[middle]+ list_at_step[middle-1])/2
                else:
                    median = list_at_step[0]
                # print(median)
                processed[c, 1] = list_at_step[i-1] - list_at_step[0]
                processed[c, 2] = list_at_step[i - 1] - median


            processed[:,0] = times
            #Writing the DF
            cols = ["Times", "Diff", "Median"]
            df = pd.DataFrame(processed[0:length,:], columns= cols )
            df.to_csv(minmax_filename.format(i,j))



def process_data_staleness(Agents, repeats):
    """
    Depricated: This data is not gathered by the simulator it self and stored in staleness... file
    Takes the data generated by the process_data_line function calculates the staleness of every update.
    Also only  use for Hogwild, relies on there only being one update a time!!
    """
    #Each Iteration is processed on each own
    for i in range(0, repeats):
        for j in Agents:
            list_of_lists = []
            data = pd.read_csv("Processed:nAgents="+str(j)+":itr="+str(i)+".log")
            # get the Time data form the DataFrame as an numpy array
            times = data.get_values()[:, 1]
            for k in range(2, j + 2):
                list_of_lists.append(data.get_values()[:, k])  # inefficent always calling this!
            #staleness for each numpy array we collected above
            staleness = [0] * j
            # nr of updates so far
            update_nr = [0] * j

            length = list_of_lists[0].shape[0]
            max_ = (list_of_lists[0])[length-1,2]
            size = j*(max_+1)
            processed = np.zeros((size, 2))


            counter = 0
            for c in range(0, length):
                for a,l in enumerate(list_of_lists):
                    if l[c] != update_nr[c]:
                        # update log
                        processed[counter,0] = a
                        processed[counter,1] = staleness[a]

                        update_nr[c] = l[c]
                        for s in range(0, len(staleness)):
                            staleness[s] += 1

                        staleness[c] = 0

                        break
                    else:
                        pass

            #Writing the DF
            cols = ["Time"].append(["Agent"+str(nr)+"_Itr" for nr in range(0,j)])
            df = pd.DataFrame(processed[0:size,:], columns= cols )
            df.to_csv("Staleness:nAgents="+str(j)+":"+str(i)+".log")





def plot_line(Agents, repeats):
    """
    Depricated
    Note:   The graph generated by this function did not prove to be usefull.
            Therefore this function hasn't been used alot, No guarantee it actually still works.
    Plots the updates over time for each agent, uses the data generated by process_data_line function.
    """
    for j in Agents:
        res = []
        for i in range(0, repeats):
            data = pd.read_csv("Processed:nAgents="+str(j)+":"+str(i)+".log")
            res.append(data.get_values()[:, 1:j+2])
        # ensemble.append(res1)

        length_sum = 0
        #not every Processed file has the same length!
        for r in res:
            length_sum += r.shape[0]
        print(length_sum)
        plt.figure(figsize=[8,4])

        plt.subplot(111)


        x = np.zeros([length_sum*j]) #X coordinates
        h = np.zeros([length_sum*j]) #hue
        y = np.zeros([length_sum*j]) #Y coordinates
        print(len(res))
        print(res[0].shape)
        print(x.shape)
        tmp=0

        for i in range(0, repeats):
            for l in range(0, j):
                length = res[i].shape[0]
                tmp += length
                print((res[i])[:,0].shape)
                x[tmp - length: tmp] = (res[i])[:,0]
                for k in range(0,length):
                    sum = tmp - length
                    h[sum+k] = l
                y[tmp - length :tmp] = (res[i])[:,l+1]

        plt.title("Distribution of the Updates")
        plt_sb = sb.lineplot(x=x, y=y, hue=h, ci="sd") # palette=["b", "g"]
        agent = [str(i)+"th. Agent" for i in range(1,j+1)]
        plt.legend(agent, loc="center right")
        plt.xlabel("Seconds")
        plt.ylabel("Updates")

        plt.show()

def plot(Agents, repeats, minmax_filename):
    """
         Plots the data from the process_data_minmax and process_data_median function (need to be calld in this order).
    :param Agents:
    :param repeats:
    :param minmax_filename:
    :return:
    """
    for j in Agents:
        res = []
        for i in range(0, repeats):
            data = pd.read_csv(minmax_filename.format(j,i))
            res.append(data.get_values()[:, 1:4])
        # ensemble.append(res1)

        length_sum = 0
        #not every Processed file has the same length!
        for r in res:
            length_sum += r.shape[0]
        print(length_sum)

        x = np.zeros([length_sum*2])
        h = np.ones([length_sum*2])
        y = np.zeros([length_sum*2])
        print(len(res))
        print(res[0].shape)
        print(x.shape)
        tmp=0

        for i in range(0, repeats):
            length = res[i].shape[0]
            tmp += length
            print((res[i])[:,0].shape)
            h[tmp - length: tmp] = np.ones([length])*1
            x[tmp - length: tmp] = (res[i])[:,0]
            y[tmp - length: tmp] = (res[i])[:, 1]
            h[length_sum+ tmp - length: length_sum + tmp] = np.ones([length])*2
            x[length_sum+ tmp - length: length_sum + tmp] = (res[i])[:,0]
            y[length_sum + tmp - length: length_sum+ tmp] = (res[i])[:, 2]

        plt_sb = sb.lineplot(x=x, y=y, hue=h, ci="sd") # palette=["b", "g"]
        plt.legend(["MAX - MIN", "Max - Median"], loc="center right")
        plt.xlabel("Seconds")
        plt.ylabel("Updates difference")



if __name__ == "__main__":
    filenames = [
         "Softsync_mcasS_{1}_times_8_scale=8_#agents={0}:itr=0:spcl=False_mom:agentNr={2}.log",
         "Softsync_mcasSawr_{1}_times_8_scale=1_#agents={0}:itr=0:spcl=True_mom:agentNr={2}.log",
         "Softsync_mcasSawr2_{1}_times_8_scale=1_#agents={0}:itr=0:spcl=True_mom:agentNr={2}.log"]

    endings = ["mcasS.log","mcaSawr.log", "mcasSawr.log"]
    processed_filenames = ["Processed:nAgents={0}:{1}:"+ end for end in endings]
    minmax_filenames = ["MINMAX:nAgents={0}:{1}:"+end for end in endings]
    titels = ["Asynchronous Mc","Asynchronous MC Stalenessware", "Asynchronous MC new Stalenessware"]
    num_agents = 8
    Agents = [8]
    repeats = 5
    Interval = 0.05
    for i, name in enumerate(filenames):
        process_data_line(num_agents, repeats, Interval , name, processed_filenames[i])
        process_data_minmax(Agents, repeats, processed_filenames[i], minmax_filenames[i])
        process_data_median(Agents, repeats, processed_filenames[i], minmax_filenames[i])
    gs = gridspec.GridSpec(2, 4)
    plt.figure(figsize=[12, 6])
    dims = [gs[0, 0:2], gs[0, 2:4], gs[1, 1:3]]
    for i, name in enumerate(minmax_filenames):
        # easiest way to center the last row
        # https://stackoverflow.com/questions/52014678/aligning-a-row-of-plots-in-matplotlib
        plt.subplot(dims[i])
        plt.title(titels[i])
        plot(Agents, repeats, name)
    plt.tight_layout()
    plt.show()